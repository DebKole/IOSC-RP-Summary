<table class="dataframe pdf-table">
  <thead>
    <tr style="text-align: right;">
      <th>or English open-ended evaluation, we use the MT-Bench benchmark (Zheng et al., 2023), which</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ontains 8 different categories of multi-turn questions. As illustrated in Table 8, our DeepSeek</td>
    </tr>
    <tr>
      <td>LM 67B Chat outperforms other open-source models such as LLaMA-2-Chat Touvron et al.</td>
    </tr>
    <tr>
      <td>2023b) 70B, Xwin 70b v0.1, and TÜLU 2+DPO 70B (Ivison et al., 2023), and achieves 8.35 score</td>
    </tr>
    <tr>
      <td>omparable with GPT-3.5-turbo. Besides, after the DPO stage, our DeepSeek LLM 67B Chat</td>
    </tr>
    <tr>
      <td>PO further improves the average score to 8.76, which is only behind GPT-4 (OpenAI, 2023).</td>
    </tr>
    <tr>
      <td>hese results illustrate the strong multi-turn open-ended generation ability of DeepSeek LLM.</td>
    </tr>
    <tr>
      <td>Model STEM Humanities Reasoning Coding Math Extraction Roleplay Writing Average</td>
    </tr>
    <tr>
      <td>GPT-4-1106-preview? 9.90 9.95 8.10 9.05 7.95 9.90 9.50 9.70 9.26</td>
    </tr>
    <tr>
      <td>GPT-3.5-turbo-0613? 9.55 9.95 6.20 7.05 7.05 9.00 8.65 9.65 8.39</td>
    </tr>
    <tr>
      <td>LLAMA-2-Chat 7B? 8.65 8.75 4.25 3.00 2.40 6.50 7.70 8.90 6.27</td>
    </tr>
    <tr>
      <td>LLAMA-2-Chat 13B? 8.63 9.75 5.10 3.00 3.45 6.93 7.50 8.85 6.65</td>
    </tr>
    <tr>
      <td>LLAMA-2-Chat 70B? 8.93 9.63 5.80 3.15 3.30 7.25 7.50 9.30 6.86</td>
    </tr>
    <tr>
      <td>Zephyr-Beta 7B? 9.03 9.63 5.60 5.10 4.45 7.45 8.20 9.35 7.35</td>
    </tr>
    <tr>
      <td>Xwin 70b v0.1? 9.68 9.95 6.55 4.25 3.30 8.75 8.25 9.55 7.53</td>
    </tr>
    <tr>
      <td>Xwin 13b v0.2? 9.55 9.88 5.20 3.60 2.85 7.70 8.60 8.68 7.01</td>
    </tr>
    <tr>
      <td>TÜLU 2+DPO 70B? 9.00 9.90 7.00 4.70 4.65 9.35 9.25 9.25 7.89</td>
    </tr>
    <tr>
      <td>DeepSeek LLM 67B Chat 9.60 9.70 8.00 7.35 6.25 8.40 8.20 9.30 8.35</td>
    </tr>
    <tr>
      <td>DeepSeek LLM 67B Chat DPO 9.70 9.80 9.05 6.75 6.65 9.30 9.10 9.75 8.76</td>
    </tr>
    <tr>
      <td>Table 8 | MT-Bench Evaluation. Results with ? are reported in Ivison et al. (2023)</td>
    </tr>
  </tbody>
</table>